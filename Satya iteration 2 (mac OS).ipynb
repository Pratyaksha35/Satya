{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                  # For file path operations\n",
    "import csv                                 # For CSV reading/writing\n",
    "import io                                  # For in-memory binary streams\n",
    "import whisper                             # For transcription using the Whisper model\n",
    "import sounddevice as sd                   # For audio recording from the microphone\n",
    "import numpy as np                         # For numerical operations (arrays)\n",
    "import time                                # For timekeeping and delays\n",
    "import webrtcvad                           # For Voice Activity Detection (VAD)\n",
    "import re                                  # For regular expression text cleaning\n",
    "import pandas as pd                        # For DataFrame operations (CSV processing)\n",
    "import requests                            # For HTTP requests (LLM API calls)\n",
    "from concurrent.futures import ThreadPoolExecutor  # For concurrent processing (used later)\n",
    "import simpleaudio as sa                   # For audio playback\n",
    "import soundfile as sf                     # For reading audio files\n",
    "import threading                           # For multithreading (monitoring mic)\n",
    "from TTS.api import TTS                    # For text-to-speech conversion\n",
    "import queue                               # For thread-safe task queue\n",
    "\n",
    "########################################\n",
    "# Global Settings and Utility Functions\n",
    "########################################\n",
    "\n",
    "# Define the CSV file path on the Desktop\n",
    "csv_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"transcriptions.csv\")\n",
    "\n",
    "# Create a lock for thread-safe CSV writes\n",
    "csv_lock = threading.Lock()\n",
    "\n",
    "# (The transcription_queue and background worker below remain for legacy or future use.)\n",
    "transcription_queue = queue.Queue()\n",
    "\n",
    "# Initialize the Whisper model for transcription\n",
    "whisper_model = whisper.load_model(\"small\")\n",
    "sd.default.device = 0  # Use the default microphone for recording\n",
    "\n",
    "# Setup WebRTC VAD for transcription (and reuse for playback later)\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(2)  # Set VAD sensitivity (0 = least sensitive, 3 = most sensitive)\n",
    "\n",
    "# Audio recording settings\n",
    "SAMPLE_RATE = 16000                      # Audio sample rate in Hz\n",
    "FRAME_DURATION = 30                      # Frame duration in milliseconds\n",
    "FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION / 1000)  # Number of samples per frame\n",
    "\n",
    "def is_speech(frame):\n",
    "    \"\"\"\n",
    "    Check if the given audio frame contains speech using VAD.\n",
    "    \"\"\"\n",
    "    return vad.is_speech(frame.tobytes(), SAMPLE_RATE)\n",
    "\n",
    "def record_audio(timeout=None):\n",
    "    \"\"\"\n",
    "    Record audio from the microphone until a period of silence is detected.\n",
    "    If a timeout is provided and no speech is detected within that time (before recording starts),\n",
    "    returns None.\n",
    "    \"\"\"\n",
    "    print(\"ðŸŽ¤ Transcription Phase: Listening...\")\n",
    "    buffer = []              # To store recorded frames\n",
    "    silence_count = 0        # Counter for consecutive silent frames\n",
    "    recording = False        # Flag to indicate if recording has started\n",
    "    start_time = time.time() if timeout is not None else None\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, dtype=\"int16\") as stream:\n",
    "        while True:\n",
    "            if timeout is not None and not recording and (time.time() - start_time > timeout):\n",
    "                print(\"â° Timeout reached without speech detection.\")\n",
    "                return None\n",
    "            frame, _ = stream.read(FRAME_SIZE)\n",
    "            frame = frame[:, 0]  # Convert stereo to mono if needed\n",
    "            if is_speech(frame):\n",
    "                buffer.append(frame)\n",
    "                silence_count = 0\n",
    "                if not recording:\n",
    "                    print(\"ðŸŽ™ï¸ Speech detected, recording...\")\n",
    "                    recording = True\n",
    "            elif recording:\n",
    "                silence_count += 1\n",
    "                # Approximately 3 seconds of silence (3 sec / 0.03 sec per frame = ~100 frames)\n",
    "                if silence_count > 100:\n",
    "                    print(\"ðŸ”‡ Speech ended, processing segment...\")\n",
    "                    break\n",
    "    if buffer:\n",
    "        return np.concatenate(buffer).astype(np.float32) / 32768.0\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean input text by removing unsupported characters and ensuring sufficient content.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^\\w\\s,.!?-]', '', text)\n",
    "    if len(cleaned.split()) < 3:\n",
    "        return \"Please provide more details.\"\n",
    "    return cleaned\n",
    "\n",
    "########################################\n",
    "# Utility Function for Dynamic RMS Filtering (Removing Long Silences Only)\n",
    "########################################\n",
    "\n",
    "def filter_audio_by_rms(audio, frame_size=FRAME_SIZE, ema_alpha=0.1, multiplier=0.5, allowed_silence_frames=3):\n",
    "    \"\"\"\n",
    "    Processes the audio frame-by-frame and removes long periods of silence.\n",
    "    For each frame, an exponential moving average (EMA) of the RMS values is computed.\n",
    "    The dynamic threshold for each frame is set to (EMA * multiplier).\n",
    "\n",
    "    Frames with RMS above the dynamic threshold are kept.\n",
    "    For frames below the threshold, only up to allowed_silence_frames consecutive silent frames are preserved.\n",
    "    This removes long periods of background noise while preserving short pauses.\n",
    "    \"\"\"\n",
    "    processed_frames = []\n",
    "    silence_buffer = []\n",
    "    ema = None  # Initialize EMA\n",
    "    \n",
    "    for i in range(0, len(audio), frame_size):\n",
    "        frame = audio[i:i+frame_size]\n",
    "        rms = np.sqrt(np.mean(frame**2))\n",
    "        # Initialize or update EMA\n",
    "        if ema is None:\n",
    "            ema = rms\n",
    "        else:\n",
    "            ema = ema_alpha * rms + (1 - ema_alpha) * ema\n",
    "        \n",
    "        # Dynamic threshold is a fraction (multiplier) of the current EMA value.\n",
    "        dynamic_threshold = ema * multiplier\n",
    "        \n",
    "        if rms >= dynamic_threshold:\n",
    "            # If there is any buffered silence, retain up to allowed silence frames\n",
    "            if silence_buffer:\n",
    "                processed_frames.extend(silence_buffer[-allowed_silence_frames:])\n",
    "                silence_buffer = []\n",
    "            processed_frames.append(frame)\n",
    "        else:\n",
    "            silence_buffer.append(frame)\n",
    "    \n",
    "    # At the end, add up to allowed silence frames if any remain\n",
    "    if silence_buffer:\n",
    "        processed_frames.extend(silence_buffer[-allowed_silence_frames:])\n",
    "    if processed_frames:\n",
    "        return np.concatenate(processed_frames)\n",
    "    return np.array([], dtype=np.float32)\n",
    "\n",
    "########################################\n",
    "# (Legacy) Background Transcription Worker\n",
    "########################################\n",
    "\n",
    "def transcription_worker():\n",
    "    \"\"\"\n",
    "    Background worker that continuously processes audio segments from the queue.\n",
    "    For each segment, it performs transcription and writes the result to the CSV.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        segment = transcription_queue.get()\n",
    "        if segment is None:\n",
    "            break  # Sentinel to shut down\n",
    "        result = whisper_model.transcribe(segment, fp16=False)\n",
    "        transcription = result.get(\"text\", \"\").strip()\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with csv_lock:\n",
    "            with open(csv_path, \"a\", newline=\"\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([timestamp, transcription, False])\n",
    "        print(\"ðŸ“ Transcription:\", transcription, \"\\n\")\n",
    "        transcription_queue.task_done()\n",
    "\n",
    "worker_thread = threading.Thread(target=transcription_worker, daemon=True)\n",
    "worker_thread.start()\n",
    "\n",
    "########################################\n",
    "# Phase 1: Transcription (Modified for Concurrent Processing)\n",
    "########################################\n",
    "\n",
    "def transcribe_segment(segment):\n",
    "    \"\"\"Helper function to transcribe a given audio segment using the Whisper model.\"\"\"\n",
    "    result = whisper_model.transcribe(segment, fp16=False)\n",
    "    return result.get(\"text\", \"\").strip()\n",
    "\n",
    "def transcribe_audio(max_duration=60):\n",
    "    \"\"\"\n",
    "    Run the transcription phase for up to max_duration seconds.\n",
    "    As soon as an audio segment is captured, it is submitted for transcription concurrently.\n",
    "    The system does not wait for the previous transcription to finish before starting to listen for the next segment.\n",
    "    Once a session ends (3 seconds of silence), all concurrent transcriptions are awaited,\n",
    "    and the non-blank results are joined as a single paragraph and written to CSV.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create CSV file with header if it doesn't exist\n",
    "    if not os.path.exists(csv_path):\n",
    "        with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\"Timestamp\", \"Transcription\", \"Played\"])\n",
    "    \n",
    "    # List to hold futures for concurrent transcription jobs\n",
    "    transcription_futures = []\n",
    "    \n",
    "    # Use a ThreadPoolExecutor for concurrent transcription\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Capture primary audio segment and submit for transcription immediately\n",
    "        primary_audio = record_audio(timeout=None)\n",
    "        if primary_audio is not None:\n",
    "            filtered_primary = filter_audio_by_rms(primary_audio)\n",
    "            if filtered_primary.size != 0:\n",
    "                transcription_futures.append(executor.submit(transcribe_segment, filtered_primary))\n",
    "            else:\n",
    "                print(\"No frames passed the RMS filter for the primary segment; skipping.\")\n",
    "        else:\n",
    "            print(\"No primary audio captured.\")\n",
    "        \n",
    "        # Immediately start listening for additional speech\n",
    "        additional_audio = record_audio(timeout=3)\n",
    "        if additional_audio is not None:\n",
    "            filtered_additional = filter_audio_by_rms(additional_audio)\n",
    "            if filtered_additional.size != 0:\n",
    "                transcription_futures.append(executor.submit(transcribe_segment, filtered_additional))\n",
    "            else:\n",
    "                print(\"No frames passed the RMS filter for the additional segment; ending session.\")\n",
    "        else:\n",
    "            print(\"ðŸ›‘ Silence detected for 3 seconds. Ending transcription session.\")\n",
    "    \n",
    "        # At this point, additional segments could be added similarly if desired.\n",
    "    \n",
    "        # Wait for all transcription jobs to complete\n",
    "        transcribed_segments = []\n",
    "        for future in transcription_futures:\n",
    "            transcription = future.result()\n",
    "            if transcription.strip():\n",
    "                transcribed_segments.append(transcription)\n",
    "            else:\n",
    "                print(\"A segment produced a blank transcription; skipping.\")\n",
    "    \n",
    "    # Aggregate all non-blank transcriptions into one paragraph\n",
    "    aggregated_text = \" \".join(seg for seg in transcribed_segments if seg.strip())\n",
    "    if aggregated_text:\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with csv_lock:\n",
    "            with open(csv_path, \"a\", newline=\"\") as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([timestamp, aggregated_text, False])\n",
    "        print(\"ðŸ“ Aggregated Transcription:\", aggregated_text, \"\\n\")\n",
    "    else:\n",
    "        print(\"No valid transcriptions captured in this session.\")\n",
    "\n",
    "########################################\n",
    "# Phase 2: LLM Response Generation (with Conciseness Step)\n",
    "########################################\n",
    "\n",
    "def run_llm_response_generation():\n",
    "    \"\"\"\n",
    "    Reads the CSV, processes transcriptions in two steps:\n",
    "      1. Runs the aggregated transcription through an LLM to make it concise while preserving its meaning.\n",
    "      2. Sends the concise transcription to the assistant prompt LLM to generate a final response.\n",
    "    The CSV is then updated with the assistant responses.\n",
    "    \"\"\"\n",
    "    gemma2b_local_api = \"http://192.168.0.193:1234/v1/chat/completions\"\n",
    "    \n",
    "    def make_concise(transcription):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert summarizer. Make the following transcription concise while preserving its meaning.\"},\n",
    "            {\"role\": \"user\", \"content\": transcription}\n",
    "        ]\n",
    "        payload = {\n",
    "            \"model\": \"gemma-2-2b-it\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 300,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(gemma2b_local_api, headers={\"Content-Type\": \"application/json\"}, json=payload)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error generating concise transcription for: '{transcription}': {e}\")\n",
    "            return transcription  # Fallback to original if error occurs\n",
    "        data = response.json()\n",
    "        try:\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        except (KeyError, IndexError):\n",
    "            return transcription\n",
    "\n",
    "    def generate_assistant_response(transcription):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are Satya, a personal assistant. Provide a concise, helpful, and friendly response. \"\n",
    "                \"Do not include any emojis in the output. There should be no symbols or special characters in your output.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": transcription}\n",
    "        ]\n",
    "        payload = {\n",
    "            \"model\": \"gemma-2-2b-it\",\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 500,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(gemma2b_local_api, headers={\"Content-Type\": \"application/json\"}, json=payload)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error generating assistant response for transcription: '{transcription}': {e}\")\n",
    "            return \"Error generating response.\"\n",
    "        data = response.json()\n",
    "        try:\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        except (KeyError, IndexError):\n",
    "            return \"No response generated.\"\n",
    "\n",
    "    def process_transcription(transcription):\n",
    "        concise = make_concise(transcription)\n",
    "        return generate_assistant_response(concise)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[\"Transcription\"])\n",
    "    if \"AssistantResponse\" in df.columns:\n",
    "        df_to_process = df[df[\"AssistantResponse\"].isnull() | (df[\"AssistantResponse\"] == \"\")]\n",
    "    else:\n",
    "        df_to_process = df\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        responses = list(executor.map(process_transcription, df_to_process[\"Transcription\"]))\n",
    "    df.loc[df_to_process.index, \"AssistantResponse\"] = responses\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"LLM Response Generation: Completed and CSV updated.\")\n",
    "\n",
    "########################################\n",
    "# Phase 3: TTS Playback\n",
    "########################################\n",
    "\n",
    "tts_playback = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=False)\n",
    "vad_playback = webrtcvad.Vad()\n",
    "vad_playback.set_mode(2)\n",
    "global_stop = False\n",
    "monitor_stop_event = threading.Event()\n",
    "\n",
    "def monitor_continuous(amp_threshold=2100, required_consecutive=5):\n",
    "    global global_stop\n",
    "    speech_counter = 0\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, dtype=\"int16\") as stream:\n",
    "        while not monitor_stop_event.is_set():\n",
    "            frame, _ = stream.read(FRAME_SIZE)\n",
    "            mic_frame = frame[:, 0]\n",
    "            amplitude = np.abs(mic_frame).mean()\n",
    "            if amplitude > amp_threshold and vad_playback.is_speech(mic_frame.tobytes(), SAMPLE_RATE):\n",
    "                speech_counter += 1\n",
    "                if speech_counter >= required_consecutive:\n",
    "                    print(\"Continuous user speech detected. Interrupting TTS phase.\")\n",
    "                    global_stop = True\n",
    "                    monitor_stop_event.set()\n",
    "                    break\n",
    "            else:\n",
    "                speech_counter = 0\n",
    "            time.sleep(0.01)\n",
    "\n",
    "def monitor_mic(play_obj, amp_threshold=2100, required_consecutive=5):\n",
    "    global global_stop\n",
    "    speech_counter = 0\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, dtype=\"int16\") as stream:\n",
    "        while play_obj.is_playing():\n",
    "            frame, _ = stream.read(FRAME_SIZE)\n",
    "            mic_frame = frame[:, 0]\n",
    "            amplitude = np.abs(mic_frame).mean()\n",
    "            if amplitude > amp_threshold and vad_playback.is_speech(mic_frame.tobytes(), SAMPLE_RATE):\n",
    "                speech_counter += 1\n",
    "                if speech_counter >= required_consecutive:\n",
    "                    print(\"User speech detected during clip playback. Stopping playback.\")\n",
    "                    play_obj.stop()\n",
    "                    global_stop = True\n",
    "                    break\n",
    "            else:\n",
    "                speech_counter = 0\n",
    "            time.sleep(0.01)\n",
    "\n",
    "def run_tts_playback():\n",
    "    global global_stop\n",
    "    monitor_stop_event.clear()\n",
    "    continuous_monitor_thread = threading.Thread(target=monitor_continuous)\n",
    "    continuous_monitor_thread.start()\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=[\"AssistantResponse\"])\n",
    "    if \"Played\" not in df.columns:\n",
    "        df[\"Played\"] = False\n",
    "        \n",
    "    for idx, row in df.iterrows():\n",
    "        if global_stop:\n",
    "            print(\"User speech detected; marking remaining rows as played.\")\n",
    "            df.loc[idx:, \"Played\"] = True\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            break\n",
    "        if row[\"Played\"]:\n",
    "            print(f\"Skipping row {idx} as it has already been played.\")\n",
    "            continue\n",
    "        text = clean_text(str(row[\"AssistantResponse\"]).strip())\n",
    "        if not text:\n",
    "            print(f\"Skipping row {idx} due to empty response after cleaning\")\n",
    "            continue\n",
    "        print(f\"Speaking for row {idx}: {text}\")\n",
    "        try:\n",
    "            audio_buffer = io.BytesIO()\n",
    "            tts_playback.tts_to_file(text=text, file_path=audio_buffer)\n",
    "            audio_buffer.seek(0)\n",
    "            data, samplerate = sf.read(audio_buffer, dtype='int16')\n",
    "            audio_array = np.array(data, dtype=np.int16).tobytes()\n",
    "            wave_obj = sa.WaveObject(audio_array, num_channels=1, bytes_per_sample=2, sample_rate=samplerate)\n",
    "            play_obj = wave_obj.play()\n",
    "            monitor_thread = threading.Thread(target=monitor_mic, args=(play_obj,))\n",
    "            monitor_thread.start()\n",
    "            play_obj.wait_done()\n",
    "            monitor_thread.join()\n",
    "            if global_stop:\n",
    "                print(\"User speech detected during playback; marking remaining rows as played.\")\n",
    "                df.loc[idx:, \"Played\"] = True\n",
    "                df.to_csv(csv_path, index=False)\n",
    "                break\n",
    "            df.at[idx, \"Played\"] = True\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    monitor_stop_event.set()\n",
    "    continuous_monitor_thread.join()\n",
    "\n",
    "########################################\n",
    "# Main Execution Pipeline (Continuous Loop)\n",
    "########################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        while True:\n",
    "            print(\"\\n--- Starting Transcription Phase ---\")\n",
    "            transcribe_audio(max_duration=60)\n",
    "            \n",
    "            print(\"\\n--- Running LLM Response Generation Phase ---\")\n",
    "            run_llm_response_generation()\n",
    "            \n",
    "            global_stop = False\n",
    "            print(\"\\n--- Running TTS Playback Phase ---\")\n",
    "            run_tts_playback()\n",
    "            \n",
    "            print(\"\\n--- Cycle complete. Restarting in 1 second... ---\\n\")\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nðŸ›‘ Transcription interrupted by user.\")\n",
    "        transcription_queue.put(None)\n",
    "        worker_thread.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
